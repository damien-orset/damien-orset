---
lang: us
ref: enedis

title: Enedis
type: project
client: Enedis
job: Expert Technique 
date_start: 2016-01-01
date_end: 2017-09-31
description: "Enedis B4All, Hadoop & Middleware Expertise"
technos:
  - Hortonworks
---
# My mission

Support, advice, technical expertise around the hadoop stack
- Industrialization of installation of HDP 2.5 clusters (bash via API or Blueprint Ambari)
- Installation of Edge Nodes for data scientist (ansible) and associated configuration of the cluster: creation, partitioning, optimization and securing of the resources made available by the cluster (YARN, Ranger, Jupyter). IaaS used: OpenStack
- Implementation of an hdfs backup mechanism via snapshots and distcp
- Ingestion in the DataLake thanks to a Kafka Confluent cluster (Connect/Stream/Schema Registry)
- Implementation of data security with Kerberos, Knox, Ranger and HDFS ACLs
- Data governance: POC on Atlas and Falcon
Support activity (SN3), consulting, audit, engineering on middleware products (web/application server, portal, MOM, APM, etc.)
- Study, technology watch, management of the aborescence around the solutions (ex: implementation of a decision support tool for migration from Weblogic server to JBoss or Tomcat)
- Industrialization and reliability of operation and deployments: creation of application "strains" (bash/rpm) that repackage publisher products, automated deployment scripts (installation/instantiation) of products
- Securing infrastructure and application layers: patching scripts (CVE), migrations
- Performance optimization: jvm tuning, heap dump analysis, gc logs, etc.

# Examples of my work

# Technical environnement
Hadoop (HDP 2.5 : Ambari, Yarn, Ranger, Spark, MapReduce, Tez, Jupyter, Zepelin, Hue, Hive, HBase, Pig, Kafka, Flume, Sqoop, Phoenix, Kerberos, Oozie, Atlas, Falcon, Zookeeper, ...), Apache httpd, Tomcat, Weblogic Server/SOA/BPM, JBoss, IIS, ActiveMQ, RabbitMQ, Liferay, SolR Docker, CA Introscope, Git, GitLAB, Ansible